var __knownSymbol = (name, symbol) => (symbol = Symbol[name]) ? symbol : Symbol.for("Symbol." + name);
var __await = function(promise, isYieldStar) {
  this[0] = promise;
  this[1] = isYieldStar;
};
var __asyncGenerator = (__this, __arguments, generator) => {
  var resume = (k, v, yes, no) => {
    try {
      var x = generator[k](v), isAwait = (v = x.value) instanceof __await, done = x.done;
      Promise.resolve(isAwait ? v[0] : v).then((y) => isAwait ? resume(k === "return" ? k : "next", v[1] ? { done: y.done, value: y.value } : y, yes, no) : yes({ value: y, done })).catch((e) => resume("throw", e, yes, no));
    } catch (e) {
      no(e);
    }
  }, method = (k) => it[k] = (x) => new Promise((yes, no) => resume(k, x, yes, no)), it = {};
  return generator = generator.apply(__this, __arguments), it[__knownSymbol("asyncIterator")] = () => it, method("next"), method("throw"), method("return"), it;
};
/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */
import { logger } from "../utils/logger.js";
class GeminiLlmConnection {
  constructor(geminiSession) {
    this.geminiSession = geminiSession;
  }
  /**
   * Sends the conversation history to the gemini model.
   *
   * You call this method right after setting up the model connection.
   * The model will respond if the last content is from user, otherwise it will
   * wait for new user input before responding.
   *
   * @param history The conversation history to send to the model.
   */
  async sendHistory(history) {
    const contents = history.filter(
      (content) => {
        var _a;
        return content.parts && ((_a = content.parts[0]) == null ? void 0 : _a.text);
      }
    );
    if (contents.length > 0) {
      this.geminiSession.sendClientContent({
        turns: contents,
        turnComplete: contents[contents.length - 1].role === "user"
      });
    } else {
      logger.info("no content is sent");
    }
  }
  /**
   * Sends a user content to the gemini model.
   *
   * The model will respond immediately upon receiving the content.
   * If you send function responses, all parts in the content should be function
   * responses.
   *
   * @param content The content to send to the model.
   */
  async sendContent(content) {
    if (!content.parts) {
      throw new Error("Content must have parts.");
    }
    if (content.parts[0].functionResponse) {
      const functionResponses = content.parts.map((part) => part.functionResponse).filter((fr) => !!fr);
      logger.debug("Sending LLM function response:", functionResponses);
      this.geminiSession.sendToolResponse({
        functionResponses
      });
    } else {
      logger.debug("Sending LLM new content", content);
      this.geminiSession.sendClientContent({
        turns: [content],
        turnComplete: true
      });
    }
  }
  /**
   * Sends a chunk of audio or a frame of video to the model in realtime.
   *
   * @param blob The blob to send to the model.
   */
  async sendRealtime(blob) {
    logger.debug("Sending LLM Blob:", blob);
    this.geminiSession.sendRealtimeInput({ media: blob });
  }
  /**
   * Builds a full text response.
   *
   * The text should not be partial and the returned LlmResponse is not be
   * partial.
   *
   * @param text The text to be included in the response.
   * @returns An LlmResponse containing the full text.
   */
  buildFullTextResponse(text) {
    return {
      content: {
        role: "model",
        parts: [{ text }]
      }
    };
  }
  // TODO(b/425992518): GenAI SDK inconsistent API, missing methods.
  receive() {
    return __asyncGenerator(this, null, function* () {
      throw new Error("Not Implemented.");
    });
  }
  /**
   * Closes the llm server connection.
   */
  async close() {
    this.geminiSession.close();
  }
}
export {
  GeminiLlmConnection
};
