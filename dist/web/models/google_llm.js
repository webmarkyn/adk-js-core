var __defProp = Object.defineProperty;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __knownSymbol = (name, symbol) => (symbol = Symbol[name]) ? symbol : Symbol.for("Symbol." + name);
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __await = function(promise, isYieldStar) {
  this[0] = promise;
  this[1] = isYieldStar;
};
var __asyncGenerator = (__this, __arguments, generator) => {
  var resume = (k, v, yes, no) => {
    try {
      var x = generator[k](v), isAwait = (v = x.value) instanceof __await, done = x.done;
      Promise.resolve(isAwait ? v[0] : v).then((y) => isAwait ? resume(k === "return" ? k : "next", v[1] ? { done: y.done, value: y.value } : y, yes, no) : yes({ value: y, done })).catch((e) => resume("throw", e, yes, no));
    } catch (e) {
      no(e);
    }
  }, method = (k) => it[k] = (x) => new Promise((yes, no) => resume(k, x, yes, no)), it = {};
  return generator = generator.apply(__this, __arguments), it[__knownSymbol("asyncIterator")] = () => it, method("next"), method("throw"), method("return"), it;
};
var __forAwait = (obj, it, method) => (it = obj[__knownSymbol("asyncIterator")]) ? it.call(obj) : (obj = obj[__knownSymbol("iterator")](), it = {}, method = (key, fn) => (fn = obj[key]) && (it[key] = (arg) => new Promise((yes, no, done) => (arg = fn.call(obj, arg), done = arg.done, Promise.resolve(arg.value).then((value) => yes({ value, done }), no)))), method("next"), method("return"), it);
/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */
import { createPartFromText, FinishReason, GoogleGenAI } from "@google/genai";
import { logger } from "../utils/logger.js";
import { GoogleLLMVariant } from "../utils/variant_utils.js";
import { BaseLlm } from "./base_llm.js";
import { GeminiLlmConnection } from "./gemini_llm_connection.js";
import { createLlmResponse } from "./llm_response.js";
const AGENT_ENGINE_TELEMETRY_TAG = "remote_reasoning_engine";
const AGENT_ENGINE_TELEMETRY_ENV_VARIABLE_NAME = "GOOGLE_CLOUD_AGENT_ENGINE_ID";
class Gemini extends BaseLlm {
  /**
   * @param params The parameters for creating a Gemini instance.
   */
  constructor({
    model,
    apiKey,
    vertexai,
    project,
    location,
    headers
  }) {
    if (!model) {
      model = "gemini-2.5-flash";
    }
    super({ model });
    this.project = project;
    this.location = location;
    this.apiKey = apiKey;
    this.headers = headers;
    const canReadEnv = typeof process === "object";
    this.vertexai = !!vertexai;
    if (!this.vertexai && canReadEnv) {
      const vertexAIfromEnv = process.env["GOOGLE_GENAI_USE_VERTEXAI"];
      if (vertexAIfromEnv) {
        this.vertexai = vertexAIfromEnv.toLowerCase() === "true" || vertexAIfromEnv === "1";
      }
    }
    if (this.vertexai) {
      if (canReadEnv && !this.project) {
        this.project = process.env["GOOGLE_CLOUD_PROJECT"];
      }
      if (canReadEnv && !this.location) {
        this.location = process.env["GOOGLE_CLOUD_LOCATION"];
      }
      if (!this.project) {
        throw new Error(
          "VertexAI project must be provided via constructor or GOOGLE_CLOUD_PROJECT environment variable."
        );
      }
      if (!this.location) {
        throw new Error(
          "VertexAI location must be provided via constructor or GOOGLE_CLOUD_LOCATION environment variable."
        );
      }
    } else {
      if (!this.apiKey && canReadEnv) {
        this.apiKey = process.env["GOOGLE_GENAI_API_KEY"] || process.env["GEMINI_API_KEY"];
      }
      if (!this.apiKey) {
        throw new Error(
          "API key must be provided via constructor or GOOGLE_GENAI_API_KEY or GEMINI_API_KEY environment variable."
        );
      }
    }
  }
  /**
   * Sends a request to the Gemini model.
   *
   * @param llmRequest LlmRequest, the request to send to the Gemini model.
   * @param stream bool = false, whether to do streaming call.
   * @yields LlmResponse: The model response.
   */
  generateContentAsync(llmRequest, stream = false) {
    return __asyncGenerator(this, null, function* () {
      var _a, _b, _c, _d, _e, _f, _g;
      this.preprocessRequest(llmRequest);
      this.maybeAppendUserContent(llmRequest);
      logger.info(
        "Sending out request, model: ".concat(llmRequest.model, ", backend: ").concat(this.apiBackend, ", stream: ").concat(stream)
      );
      if ((_a = llmRequest.config) == null ? void 0 : _a.httpOptions) {
        llmRequest.config.httpOptions.headers = __spreadValues(__spreadValues({}, llmRequest.config.httpOptions.headers), this.trackingHeaders);
      }
      if (stream) {
        const streamResult = yield new __await(this.apiClient.models.generateContentStream({
          model: (_b = llmRequest.model) != null ? _b : this.model,
          contents: llmRequest.contents,
          config: llmRequest.config
        }));
        let thoughtText = "";
        let text = "";
        let usageMetadata;
        let lastResponse;
        try {
          for (var iter = __forAwait(streamResult), more, temp, error; more = !(temp = yield new __await(iter.next())).done; more = false) {
            const response = temp.value;
            lastResponse = response;
            const llmResponse = createLlmResponse(response);
            usageMetadata = llmResponse.usageMetadata;
            const firstPart = (_d = (_c = llmResponse.content) == null ? void 0 : _c.parts) == null ? void 0 : _d[0];
            if (firstPart == null ? void 0 : firstPart.text) {
              if ("thought" in firstPart && firstPart.thought) {
                thoughtText += firstPart.text;
              } else {
                text += firstPart.text;
              }
              llmResponse.partial = true;
            } else if ((thoughtText || text) && (!firstPart || !firstPart.inlineData)) {
              const parts = [];
              if (thoughtText) {
                parts.push({ text: thoughtText, thought: true });
              }
              if (text) {
                parts.push(createPartFromText(text));
              }
              yield {
                content: {
                  role: "model",
                  parts
                },
                usageMetadata: llmResponse.usageMetadata
              };
              thoughtText = "";
              text = "";
            }
            yield llmResponse;
          }
        } catch (temp) {
          error = [temp];
        } finally {
          try {
            more && (temp = iter.return) && (yield new __await(temp.call(iter)));
          } finally {
            if (error)
              throw error[0];
          }
        }
        if ((text || thoughtText) && ((_f = (_e = lastResponse == null ? void 0 : lastResponse.candidates) == null ? void 0 : _e[0]) == null ? void 0 : _f.finishReason) === FinishReason.STOP) {
          const parts = [];
          if (thoughtText) {
            parts.push({ text: thoughtText, thought: true });
          }
          if (text) {
            parts.push({ text });
          }
          yield {
            content: {
              role: "model",
              parts
            },
            usageMetadata
          };
        }
      } else {
        const response = yield new __await(this.apiClient.models.generateContent({
          model: (_g = llmRequest.model) != null ? _g : this.model,
          contents: llmRequest.contents,
          config: llmRequest.config
        }));
        yield createLlmResponse(response);
      }
    });
  }
  get apiClient() {
    if (this._apiClient) {
      return this._apiClient;
    }
    const combinedHeaders = __spreadValues(__spreadValues({}, this.trackingHeaders), this.headers);
    if (this.vertexai) {
      this._apiClient = new GoogleGenAI({
        vertexai: this.vertexai,
        project: this.project,
        location: this.location,
        httpOptions: { headers: combinedHeaders }
      });
    } else {
      this._apiClient = new GoogleGenAI({
        apiKey: this.apiKey,
        httpOptions: { headers: combinedHeaders }
      });
    }
    return this._apiClient;
  }
  get apiBackend() {
    if (!this._apiBackend) {
      this._apiBackend = this.apiClient.vertexai ? GoogleLLMVariant.VERTEX_AI : GoogleLLMVariant.GEMINI_API;
    }
    return this._apiBackend;
  }
  get liveApiVersion() {
    if (!this._liveApiVersion) {
      this._liveApiVersion = this.apiBackend === GoogleLLMVariant.VERTEX_AI ? "v1beta1" : "v1alpha";
    }
    return this._liveApiVersion;
  }
  get liveApiClient() {
    if (!this._liveApiClient) {
      this._liveApiClient = new GoogleGenAI({
        apiKey: this.apiKey,
        httpOptions: {
          headers: this.trackingHeaders,
          apiVersion: this.liveApiVersion
        }
      });
    }
    return this._liveApiClient;
  }
  /**
   * Connects to the Gemini model and returns an llm connection.
   *
   * @param llmRequest LlmRequest, the request to send to the Gemini model.
   * @returns BaseLlmConnection, the connection to the Gemini model.
   */
  async connect(llmRequest) {
    var _a, _b, _c, _d;
    if ((_a = llmRequest.liveConnectConfig) == null ? void 0 : _a.httpOptions) {
      if (!llmRequest.liveConnectConfig.httpOptions.headers) {
        llmRequest.liveConnectConfig.httpOptions.headers = {};
      }
      Object.assign(
        llmRequest.liveConnectConfig.httpOptions.headers,
        this.trackingHeaders
      );
      llmRequest.liveConnectConfig.httpOptions.apiVersion = this.liveApiVersion;
    }
    if ((_b = llmRequest.config) == null ? void 0 : _b.systemInstruction) {
      llmRequest.liveConnectConfig.systemInstruction = {
        role: "system",
        // TODO - b/425992518: validate type casting works well.
        parts: [createPartFromText(llmRequest.config.systemInstruction)]
      };
    }
    llmRequest.liveConnectConfig.tools = (_c = llmRequest.config) == null ? void 0 : _c.tools;
    const liveSession = await this.liveApiClient.live.connect({
      model: (_d = llmRequest.model) != null ? _d : this.model,
      config: llmRequest.liveConnectConfig,
      callbacks: {
        // TODO - b/425992518: GenAI SDK inconsistent API, missing methods.
        onmessage: () => {
        }
      }
    });
    return new GeminiLlmConnection(liveSession);
  }
  preprocessRequest(llmRequest) {
    if (this.apiBackend === GoogleLLMVariant.GEMINI_API) {
      if (llmRequest.config) {
        llmRequest.config.labels = void 0;
      }
      if (llmRequest.contents) {
        for (const content of llmRequest.contents) {
          if (!content.parts) continue;
          for (const part of content.parts) {
            removeDisplayNameIfPresent(part.inlineData);
            removeDisplayNameIfPresent(part.fileData);
          }
        }
      }
    }
  }
}
/**
 * A list of model name patterns that are supported by this LLM.
 *
 * @returns A list of supported models.
 */
Gemini.supportedModels = [
  /gemini-.*/,
  // fine-tuned vertex endpoint pattern
  /projects\/.+\/locations\/.+\/endpoints\/.+/,
  // vertex gemini long name
  /projects\/.+\/locations\/.+\/publishers\/google\/models\/gemini.+/
];
function removeDisplayNameIfPresent(dataObj) {
  if (dataObj && dataObj.displayName) {
    dataObj.displayName = void 0;
  }
}
export {
  Gemini
};
