/**
  * @license
  * Copyright 2025 Google LLC
  * SPDX-License-Identifier: Apache-2.0
  */

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var gemini_llm_connection_exports = {};
__export(gemini_llm_connection_exports, {
  GeminiLlmConnection: () => GeminiLlmConnection
});
module.exports = __toCommonJS(gemini_llm_connection_exports);
var import_logger = require("../utils/logger.js");
/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */
class GeminiLlmConnection {
  constructor(geminiSession) {
    this.geminiSession = geminiSession;
  }
  /**
   * Sends the conversation history to the gemini model.
   *
   * You call this method right after setting up the model connection.
   * The model will respond if the last content is from user, otherwise it will
   * wait for new user input before responding.
   *
   * @param history The conversation history to send to the model.
   */
  async sendHistory(history) {
    const contents = history.filter(
      (content) => {
        var _a;
        return content.parts && ((_a = content.parts[0]) == null ? void 0 : _a.text);
      }
    );
    if (contents.length > 0) {
      this.geminiSession.sendClientContent({
        turns: contents,
        turnComplete: contents[contents.length - 1].role === "user"
      });
    } else {
      import_logger.logger.info("no content is sent");
    }
  }
  /**
   * Sends a user content to the gemini model.
   *
   * The model will respond immediately upon receiving the content.
   * If you send function responses, all parts in the content should be function
   * responses.
   *
   * @param content The content to send to the model.
   */
  async sendContent(content) {
    if (!content.parts) {
      throw new Error("Content must have parts.");
    }
    if (content.parts[0].functionResponse) {
      const functionResponses = content.parts.map((part) => part.functionResponse).filter((fr) => !!fr);
      import_logger.logger.debug("Sending LLM function response:", functionResponses);
      this.geminiSession.sendToolResponse({
        functionResponses
      });
    } else {
      import_logger.logger.debug("Sending LLM new content", content);
      this.geminiSession.sendClientContent({
        turns: [content],
        turnComplete: true
      });
    }
  }
  /**
   * Sends a chunk of audio or a frame of video to the model in realtime.
   *
   * @param blob The blob to send to the model.
   */
  async sendRealtime(blob) {
    import_logger.logger.debug("Sending LLM Blob:", blob);
    this.geminiSession.sendRealtimeInput({ media: blob });
  }
  /**
   * Builds a full text response.
   *
   * The text should not be partial and the returned LlmResponse is not be
   * partial.
   *
   * @param text The text to be included in the response.
   * @returns An LlmResponse containing the full text.
   */
  buildFullTextResponse(text) {
    return {
      content: {
        role: "model",
        parts: [{ text }]
      }
    };
  }
  // TODO(b/425992518): GenAI SDK inconsistent API, missing methods.
  async *receive() {
    throw new Error("Not Implemented.");
  }
  /**
   * Closes the llm server connection.
   */
  async close() {
    this.geminiSession.close();
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  GeminiLlmConnection
});
