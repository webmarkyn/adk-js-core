/**
  * @license
  * Copyright 2025 Google LLC
  * SPDX-License-Identifier: Apache-2.0
  */

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var google_llm_exports = {};
__export(google_llm_exports, {
  Gemini: () => Gemini
});
module.exports = __toCommonJS(google_llm_exports);
var import_genai = require("@google/genai");
var import_logger = require("../utils/logger.js");
var import_variant_utils = require("../utils/variant_utils.js");
var import_base_llm = require("./base_llm.js");
var import_gemini_llm_connection = require("./gemini_llm_connection.js");
var import_llm_response = require("./llm_response.js");
/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */
const AGENT_ENGINE_TELEMETRY_TAG = "remote_reasoning_engine";
const AGENT_ENGINE_TELEMETRY_ENV_VARIABLE_NAME = "GOOGLE_CLOUD_AGENT_ENGINE_ID";
class Gemini extends import_base_llm.BaseLlm {
  /**
   * @param params The parameters for creating a Gemini instance.
   */
  constructor({
    model,
    apiKey,
    vertexai,
    project,
    location,
    headers
  }) {
    if (!model) {
      model = "gemini-2.5-flash";
    }
    super({ model });
    this.project = project;
    this.location = location;
    this.apiKey = apiKey;
    this.headers = headers;
    const canReadEnv = typeof process === "object";
    this.vertexai = !!vertexai;
    if (!this.vertexai && canReadEnv) {
      const vertexAIfromEnv = process.env["GOOGLE_GENAI_USE_VERTEXAI"];
      if (vertexAIfromEnv) {
        this.vertexai = vertexAIfromEnv.toLowerCase() === "true" || vertexAIfromEnv === "1";
      }
    }
    if (this.vertexai) {
      if (canReadEnv && !this.project) {
        this.project = process.env["GOOGLE_CLOUD_PROJECT"];
      }
      if (canReadEnv && !this.location) {
        this.location = process.env["GOOGLE_CLOUD_LOCATION"];
      }
      if (!this.project) {
        throw new Error(
          "VertexAI project must be provided via constructor or GOOGLE_CLOUD_PROJECT environment variable."
        );
      }
      if (!this.location) {
        throw new Error(
          "VertexAI location must be provided via constructor or GOOGLE_CLOUD_LOCATION environment variable."
        );
      }
    } else {
      if (!this.apiKey && canReadEnv) {
        this.apiKey = process.env["GOOGLE_GENAI_API_KEY"] || process.env["GEMINI_API_KEY"];
      }
      if (!this.apiKey) {
        throw new Error(
          "API key must be provided via constructor or GOOGLE_GENAI_API_KEY or GEMINI_API_KEY environment variable."
        );
      }
    }
  }
  /**
   * Sends a request to the Gemini model.
   *
   * @param llmRequest LlmRequest, the request to send to the Gemini model.
   * @param stream bool = false, whether to do streaming call.
   * @yields LlmResponse: The model response.
   */
  async *generateContentAsync(llmRequest, stream = false) {
    var _a, _b, _c, _d, _e, _f, _g;
    this.preprocessRequest(llmRequest);
    this.maybeAppendUserContent(llmRequest);
    import_logger.logger.info(
      `Sending out request, model: ${llmRequest.model}, backend: ${this.apiBackend}, stream: ${stream}`
    );
    if ((_a = llmRequest.config) == null ? void 0 : _a.httpOptions) {
      llmRequest.config.httpOptions.headers = {
        ...llmRequest.config.httpOptions.headers,
        ...this.trackingHeaders
      };
    }
    if (stream) {
      const streamResult = await this.apiClient.models.generateContentStream({
        model: (_b = llmRequest.model) != null ? _b : this.model,
        contents: llmRequest.contents,
        config: llmRequest.config
      });
      let thoughtText = "";
      let text = "";
      let usageMetadata;
      let lastResponse;
      for await (const response of streamResult) {
        lastResponse = response;
        const llmResponse = (0, import_llm_response.createLlmResponse)(response);
        usageMetadata = llmResponse.usageMetadata;
        const firstPart = (_d = (_c = llmResponse.content) == null ? void 0 : _c.parts) == null ? void 0 : _d[0];
        if (firstPart == null ? void 0 : firstPart.text) {
          if ("thought" in firstPart && firstPart.thought) {
            thoughtText += firstPart.text;
          } else {
            text += firstPart.text;
          }
          llmResponse.partial = true;
        } else if ((thoughtText || text) && (!firstPart || !firstPart.inlineData)) {
          const parts = [];
          if (thoughtText) {
            parts.push({ text: thoughtText, thought: true });
          }
          if (text) {
            parts.push((0, import_genai.createPartFromText)(text));
          }
          yield {
            content: {
              role: "model",
              parts
            },
            usageMetadata: llmResponse.usageMetadata
          };
          thoughtText = "";
          text = "";
        }
        yield llmResponse;
      }
      if ((text || thoughtText) && ((_f = (_e = lastResponse == null ? void 0 : lastResponse.candidates) == null ? void 0 : _e[0]) == null ? void 0 : _f.finishReason) === import_genai.FinishReason.STOP) {
        const parts = [];
        if (thoughtText) {
          parts.push({ text: thoughtText, thought: true });
        }
        if (text) {
          parts.push({ text });
        }
        yield {
          content: {
            role: "model",
            parts
          },
          usageMetadata
        };
      }
    } else {
      const response = await this.apiClient.models.generateContent({
        model: (_g = llmRequest.model) != null ? _g : this.model,
        contents: llmRequest.contents,
        config: llmRequest.config
      });
      yield (0, import_llm_response.createLlmResponse)(response);
    }
  }
  get apiClient() {
    if (this._apiClient) {
      return this._apiClient;
    }
    const combinedHeaders = {
      ...this.trackingHeaders,
      ...this.headers
    };
    if (this.vertexai) {
      this._apiClient = new import_genai.GoogleGenAI({
        vertexai: this.vertexai,
        project: this.project,
        location: this.location,
        httpOptions: { headers: combinedHeaders }
      });
    } else {
      this._apiClient = new import_genai.GoogleGenAI({
        apiKey: this.apiKey,
        httpOptions: { headers: combinedHeaders }
      });
    }
    return this._apiClient;
  }
  get apiBackend() {
    if (!this._apiBackend) {
      this._apiBackend = this.apiClient.vertexai ? import_variant_utils.GoogleLLMVariant.VERTEX_AI : import_variant_utils.GoogleLLMVariant.GEMINI_API;
    }
    return this._apiBackend;
  }
  get liveApiVersion() {
    if (!this._liveApiVersion) {
      this._liveApiVersion = this.apiBackend === import_variant_utils.GoogleLLMVariant.VERTEX_AI ? "v1beta1" : "v1alpha";
    }
    return this._liveApiVersion;
  }
  get liveApiClient() {
    if (!this._liveApiClient) {
      this._liveApiClient = new import_genai.GoogleGenAI({
        apiKey: this.apiKey,
        httpOptions: {
          headers: this.trackingHeaders,
          apiVersion: this.liveApiVersion
        }
      });
    }
    return this._liveApiClient;
  }
  /**
   * Connects to the Gemini model and returns an llm connection.
   *
   * @param llmRequest LlmRequest, the request to send to the Gemini model.
   * @returns BaseLlmConnection, the connection to the Gemini model.
   */
  async connect(llmRequest) {
    var _a, _b, _c, _d;
    if ((_a = llmRequest.liveConnectConfig) == null ? void 0 : _a.httpOptions) {
      if (!llmRequest.liveConnectConfig.httpOptions.headers) {
        llmRequest.liveConnectConfig.httpOptions.headers = {};
      }
      Object.assign(
        llmRequest.liveConnectConfig.httpOptions.headers,
        this.trackingHeaders
      );
      llmRequest.liveConnectConfig.httpOptions.apiVersion = this.liveApiVersion;
    }
    if ((_b = llmRequest.config) == null ? void 0 : _b.systemInstruction) {
      llmRequest.liveConnectConfig.systemInstruction = {
        role: "system",
        // TODO - b/425992518: validate type casting works well.
        parts: [(0, import_genai.createPartFromText)(llmRequest.config.systemInstruction)]
      };
    }
    llmRequest.liveConnectConfig.tools = (_c = llmRequest.config) == null ? void 0 : _c.tools;
    const liveSession = await this.liveApiClient.live.connect({
      model: (_d = llmRequest.model) != null ? _d : this.model,
      config: llmRequest.liveConnectConfig,
      callbacks: {
        // TODO - b/425992518: GenAI SDK inconsistent API, missing methods.
        onmessage: () => {
        }
      }
    });
    return new import_gemini_llm_connection.GeminiLlmConnection(liveSession);
  }
  preprocessRequest(llmRequest) {
    if (this.apiBackend === import_variant_utils.GoogleLLMVariant.GEMINI_API) {
      if (llmRequest.config) {
        llmRequest.config.labels = void 0;
      }
      if (llmRequest.contents) {
        for (const content of llmRequest.contents) {
          if (!content.parts) continue;
          for (const part of content.parts) {
            removeDisplayNameIfPresent(part.inlineData);
            removeDisplayNameIfPresent(part.fileData);
          }
        }
      }
    }
  }
}
/**
 * A list of model name patterns that are supported by this LLM.
 *
 * @returns A list of supported models.
 */
Gemini.supportedModels = [
  /gemini-.*/,
  // fine-tuned vertex endpoint pattern
  /projects\/.+\/locations\/.+\/endpoints\/.+/,
  // vertex gemini long name
  /projects\/.+\/locations\/.+\/publishers\/google\/models\/gemini.+/
];
function removeDisplayNameIfPresent(dataObj) {
  if (dataObj && dataObj.displayName) {
    dataObj.displayName = void 0;
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  Gemini
});
